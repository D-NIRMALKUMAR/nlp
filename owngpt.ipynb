{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = [\n",
    "    {\"context\": \"TensorFlow is an open-source platform for machine learning.\", \"question\": \"What is TensorFlow?\", \"answer\": \"an open-source platform for machine learning\"},\n",
    "    {\"context\": \"Python is a programming language.\", \"question\": \"What is Python?\", \"answer\": \"a programming language\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare dataset\n",
    "contexts = [item[\"context\"] for item in data]\n",
    "questions = [item[\"question\"] for item in data]\n",
    "answers = [item[\"answer\"] for item in data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(contexts + questions + answers)\n",
    "\n",
    "context_sequences = tokenizer.texts_to_sequences(contexts)\n",
    "question_sequences = tokenizer.texts_to_sequences(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pad sequences\n",
    "max_length = max(max(len(seq) for seq in context_sequences), max(len(seq) for seq in question_sequences))\n",
    "context_padded = pad_sequences(context_sequences, maxlen=max_length)\n",
    "question_padded = pad_sequences(question_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare answer labels as start and end indices\n",
    "start_labels = []\n",
    "end_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, answer in enumerate(answers):\n",
    "    context = contexts[i]\n",
    "    start_index = context.find(answer)\n",
    "    end_index = start_index + len(answer) - 1\n",
    "\n",
    "    # Convert to token indices\n",
    "    start_token_index = tokenizer.texts_to_sequences([context[:start_index]])[0][-1]\n",
    "    end_token_index = tokenizer.texts_to_sequences([context[:end_index + 1]])[0][-1]\n",
    "\n",
    "    start_labels.append(start_token_index)\n",
    "    end_labels.append(end_token_index)\n",
    "\n",
    "start_labels = np.array(start_labels)\n",
    "end_labels = np.array(end_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "input_context = layers.Input(shape=(max_length,))\n",
    "input_question = layers.Input(shape=(max_length,))\n",
    "\n",
    "embedding_layer = layers.Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128)\n",
    "\n",
    "context_embedding = embedding_layer(input_context)\n",
    "question_embedding = embedding_layer(input_question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge context and question\n",
    "merged = layers.concatenate([context_embedding, question_embedding])\n",
    "x = layers.LSTM(128)(merged)\n",
    "x = layers.Dense(64, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output layers for start and end positions\n",
    "start_output = layers.Dense(len(tokenizer.word_index) + 1, activation='softmax', name='start')(x)\n",
    "end_output = layers.Dense(len(tokenizer.word_index) + 1, activation='softmax', name='end')(x)\n",
    "\n",
    "model = models.Model(inputs=[input_context, input_question], outputs=[start_output, end_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with metrics for both outputs\n",
    "model.compile(optimizer='adam', \n",
    "              loss={'start': 'sparse_categorical_crossentropy', 'end': 'sparse_categorical_crossentropy'},\n",
    "              metrics={'start': 'accuracy', 'end': 'accuracy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - end_accuracy: 0.0000e+00 - loss: 5.4062 - start_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - end_accuracy: 1.0000 - loss: 5.3348 - start_accuracy: 0.5000\n",
      "Epoch 3/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - end_accuracy: 0.5000 - loss: 5.2646 - start_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 90ms/step - end_accuracy: 0.5000 - loss: 5.1788 - start_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - end_accuracy: 0.5000 - loss: 5.0705 - start_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 104ms/step - end_accuracy: 0.5000 - loss: 4.9318 - start_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - end_accuracy: 0.5000 - loss: 4.7500 - start_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - end_accuracy: 0.5000 - loss: 4.5094 - start_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - end_accuracy: 0.5000 - loss: 4.1891 - start_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - end_accuracy: 0.5000 - loss: 3.7689 - start_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1fcdbd77ca0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train the model\n",
    "model.fit([context_padded, question_padded], [start_labels, end_labels], epochs=10, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('owngpt.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to answer questions\n",
    "def answer_question(context, question):\n",
    "    context_seq = tokenizer.texts_to_sequences([context])\n",
    "    question_seq = tokenizer.texts_to_sequences([question])\n",
    "    \n",
    "    context_padded = pad_sequences(context_seq, maxlen=max_length)\n",
    "    question_padded = pad_sequences(question_seq, maxlen=max_length)\n",
    "\n",
    "    start_pred, end_pred = model.predict([context_padded, question_padded])\n",
    "    \n",
    "    start_index = np.argmax(start_pred, axis=1)[0]\n",
    "    end_index = np.argmax(end_pred, axis=1)[0]\n",
    "     # Get answer from context based on predicted indices\n",
    "    answer_tokens = context_padded[0][start_index:end_index + 1]\n",
    "    answer = tokenizer.sequences_to_texts([answer_tokens])[0]\n",
    "\n",
    "    return answer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
      "Answer: is an open source platform for machine learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "context = \"TensorFlow is an open-source platform for machine learning.\"\n",
    "question = \"What is TensorFlow?\"\n",
    "\n",
    "answer = answer_question(context, question)\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "Answer: tensorflow is a programming language\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "context = \"TensorFlow is a programming language.\"\n",
    "question = \"What is TensorFlow?\"\n",
    "\n",
    "answer = answer_question(context, question)\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
